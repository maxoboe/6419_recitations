{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient_descent.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"felCzCGSjliR"},"source":["# Visualizing Gradient Descent\n","In this notebook, we illustrate gradient descent with a one-dimensional example.\n","\n","In practice, the goal of gradient descent is to find parameter values that minimize a loss function. The loss function depends on your data and the parameters, and you generally cannot assume the shape of the loss function. \n","\n","In this example, we implement descent on a one-dimensional surface."]},{"cell_type":"code","metadata":{"id":"QBMkUsX7QnXQ"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","x = np.linspace(-3,3,100)\n","def f(x):\n","  return x**4 - 6*x**2 + x**3\n","def f_prime(x):\n","  return 4 * x**3 - 12 * x + 3 * x **2\n","y = np.array([f(_x) for _x in x])\n","dydx = np.array([f_prime(_x) for _x in x])\n","plt.plot(x,y,'k')\n","plt.plot(x, dydx,'--r')\n","plt.legend(['y','dy/dx'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIBWSz02loMP"},"source":["Note that for this example, we can analytically find the global and local minima of the function. \n","The global minimum is (-2.147, -16.306), but there's another local minimum at (1.397, -5.174). \n","The second derivative has another 0 value at (0, 0). "]},{"cell_type":"code","metadata":{"id":"n56oUP3KQ52P"},"source":["def visualize_descent(x,f,f_prime,initial_guess,stepsize=0.1,steps=1):\n","  y = np.array([f(_x) for _x in x])\n","  plt.plot(x,y)\n","  if steps ==0: return initial_guess\n","  for step in range(steps):\n","    update = stepsize * f_prime(initial_guess)\n","    updated_guess = initial_guess - update\n","    plt.plot((initial_guess, updated_guess),(f(initial_guess),f(updated_guess)), lw=3)\n","    initial_guess = updated_guess\n","  return updated_guess\n","\n","final_guess = visualize_descent(x,f,f_prime,-1,0.1,1)\n","print(final_guess)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JMaj24LbmIrW"},"source":["# Here, we show many steps of the descent algorithm, starting from this location: \n","print(visualize_descent(x,f,f_prime,1,0.1,100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBwQZtLm7xVv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mD1X9fwqmv_M"},"source":["# When we initialize in the vicinity of the global minimum, we expect to descend to the global minimum\n","print(visualize_descent(x,f,f_prime,-1,0.1,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcKKVgEVnVPS"},"source":["Why does this not converge to the correct solution? "]},{"cell_type":"code","metadata":{"id":"EF-61Vi2m8KW"},"source":["# When we initialize in the vicinity of the global minimum, we expect to descend to the global minimum\n","print(visualize_descent(x,f,f_prime,-1,0.01,100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0mvKwhmQ9hZ"},"source":["def f(x):\n","  return 1 - np.cos(x)\n","def f_prime(x):\n","  return np.sin(x)\n","visualize_descent(x,f,f_prime,2,0.1,40)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ateZ-wDrXdSr"},"source":["def f(x):\n","  return .5 * x + np.abs(1 - 2 * np.sin(x / 2))\n","def f_prime(x):\n","  return 0.5 - np.sign(1 - 2 * np.sin(x / 2)) * np.cos(x / 2)\n","visualize_descent(x,f,f_prime,2,0.1,4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vz-o5NF7UqR4"},"source":["def f(x):\n","  return .5 * x + np.abs(1 - 2 * np.sin(x**2))\n","def f_prime(x):\n","  return 0.5 + np.sign(1 - 2 * np.sin(x**2)) * -4 * x * np.cos(x ** 2)\n","visualize_descent(x,f,f_prime,2,0.1,4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdecVH2NXQ0j"},"source":[""],"execution_count":null,"outputs":[]}]}